dplyr::summarize(Mean = mean(price, na.rm=TRUE),
se = sd(price)/sqrt(n()))
stats[,2] <- lapply(stats[,2], factor)
ggplot(stats, aes_string(colnames(stats)[1], colnames(stats)[3], fill = colnames(stats)[2]))+
geom_bar(stat='identity', position = position_dodge(width=0.9), alpha=0.8)+
geom_errorbar(aes(ymin=Mean-(1.96*se),ymax=Mean+(1.96*se)),
position=position_dodge(width = 0.9), width = 0.25)+
scale_color_manual(name=dummy_lab,
values=c("#10a53dFF","#3a5e8cFF")) +
scale_fill_manual(name=dummy_lab,
values=c("#10a53dFF","#3a5e8cFF")) +
ylab('Mean Price')+
xlab(factor_lab) +
theme_bw()+
theme(panel.grid.major=element_blank(),
panel.grid.minor=element_blank(),
panel.border=element_blank(),
axis.line=element_line(),
legend.position = "top",
#legend.position = c(0.7, 0.9),
legend.box = "vertical",
legend.text = element_text(size = 5),
legend.title = element_text(size = 5, face = "bold"),
legend.key.size = unit(x = 0.4, units = "cm")
)
}
p2 <- price_diff_by_variables2(dta, "f_room_type", "f_property_type", "Room type", "Property type")
p2
p3 <- price_diff_by_variables2(dta, "f_cancellation_policy", "d_familykidfriendly", "Cancellation policy", "Family kid friendly")
p3
p4 <- price_diff_by_variables2(dta, "f_cancellation_policy", "d_tv", "Cancellation policy", "TV")
p4
#Look up property type
p5 <- price_diff_by_variables2(dta, "f_property_type", "d_cats", "Property type", "Cats")
p5
p6 <- price_diff_by_variables2(dta, "f_property_type", "d_dogs", "Property type", "Dogs")
p6
modellev1 <- " ~ n_accommodates"
# Basic Variables
basic_lev  <- c("n_accommodates", "n_beds", "f_property_type", "f_room_type", "n_days_since", "flag_days_since")
modellev2 <- paste0(" ~ ",paste(basic_lev,collapse = " + "))
# Factorized variables
basic_add <- c("f_bathroom","f_cancellation_policy","f_bed_type")
reviews <- c("f_number_of_reviews","n_review_scores_rating", "flag_review_scores_rating")
modellev3 <- paste0(" ~ ",paste(c(basic_lev, basic_add,reviews),collapse = " + "))
nonlin <- c("I(n_accommodates^2)", "I(n_days_since^2)", "I(n_days_since^3)") # Non-linearities for model 4
modellev4 <- paste0(" ~ ",paste(c(basic_lev, basic_add, reviews, nonlin), collapse = " + "))
interactions <- c("f_room_type:f_property_type","f_number_of_reviews:f_property_type") # We use : instead of * since M4 already has the other variables, we only want the interaction term
modellev5 <- paste(modellev4, "+", paste(interactions, collapse = " + "))
predictorCV <- function(regresors) {
fmla <- formula(paste0("price", regresors))
n_folds <- 5
set.seed(121212)
msecv <- list()
for (k in 1:n_folds) {
test_i <- which(folds_i == k)
data_train <- dta[-test_i, ] #  All except test_i
data_test <- dta[test_i, ]
model <- lm(fmla,data = data_train)
prediction_test <- predict(model, newdata = data_test)
msecv[[k]] <- with(data_test, mean((price - prediction_test)^2))
}
mean(do.call(rbind, msecv))
}
model_list <- list(
M1 = modellev1,
M2 = modellev2,
M3 = modellev3,
M4 = modellev4,
M5 = modellev5
)
cv_mse  <- sapply(models, predictorCV)
model_list <- list(
M1 = modellev1,
M2 = modellev2,
M3 = modellev3,
M4 = modellev4,
M5 = modellev5
)
cv_mse  <- sapply(model_list, predictorCV)
res <- data.frame(Modelo = names(cv_mse), MSE = cv_mse)
res[order(res$MSE), ] # We print the data in an ordered table
predictorCV <- function(regresors) {
fmla <- formula(paste0("price", regresors))
n_folds <- 5
set.seed(121212)
folds_i <- sample(rep(1:n_folds, length.out = nrow(dta)))
msecv <- list()
for (k in 1:n_folds) {
test_i <- which(folds_i == k)
data_train <- dta[-test_i, ] #  All except test_i
data_test <- dta[test_i, ]
model <- lm(fmla,data = data_train)
prediction_test <- predict(model, newdata = data_test)
msecv[[k]] <- with(data_test, mean((price - prediction_test)^2))
}
mean(do.call(rbind, msecv))
}
print(res[order(res$MSE), ]) # We print the data in an ordered table
predictorCV <- function(regresors) {
fmla <- formula(paste0("price", regresors))
n_folds <- 5
set.seed(121212)
folds_i <- sample(rep(1:n_folds, length.out = nrow(dta)))
msecv <- list()
for (k in 1:n_folds) {
test_i <- which(folds_i == k)
data_train <- dta[-test_i, ] #  All except test_i
data_test <- dta[test_i, ]
model <- lm(fmla,data = data_train)
prediction_test <- predict(model, newdata = data_test)
msecv[[k]] <- with(data_test, mean((price - prediction_test)^2))
}
mean(do.call(rbind, msecv))
}
model_list <- list(
M1 = modellev1,
M2 = modellev2,
M3 = modellev3,
M4 = modellev4,
M5 = modellev5
)
cv_mse  <- sapply(model_list, predictorCV)
res <- data.frame(Modelo = names(cv_mse), MSE = cv_mse)
print(res[order(res$MSE), ]) # We print the data in an ordered table
res2 <- do.call(rbind, lapply(names(model_list), function(n) {
fit <- lm(as.formula(paste("price", model_list[[n]])), data = dta)
data.frame(
Modelo       = n,
Variables    = length(attr(terms(fit), "term.labels")),
Coeficientes = sum(!is.na(coef(fit)))  # incluye intercepto
)
}))
print(res2)
model_list <- list(
M1 = modellev1,
M2 = modellev2,
M3 = modellev3,
M4 = modellev4,
M5 = modellev5
)
cv_mse  <- sapply(model_list, predictorCV)
res <- data.frame(Modelo = names(cv_mse), MSE = cv_mse)
print(res[order(res$MSE), ]) # We print the data in an ordered table
res2 <- do.call(rbind, lapply(names(model_list), function(n) {
fit <- lm(as.formula(paste("price", model_list[[n]])), data = dta)
data.frame(
Modelo       = n,
Variables    = length(attr(terms(fit), "term.labels")),
Coeficientes = sum(!is.na(coef(fit)))  # incluye intercepto
)
}))
print(res2)
# -----------------------------------------------------
# PART 1
# Exploratory and predictive analysis from observed data
# 0) Libraries and data
# 1) Description of users general behavior
# 2) Estimate effect of "sign_up" into "revenue"
# 3) Logistic regression of variables into sign_up
# r) Evaluating model's predictive capacity
# -----------------------------------------------------
# -----------------------------------------------------
# 0) Clean variables, libraries and data
# -----------------------------------------------------
# Libraries
require(pacman)
p_load(dplyr, tidyr, tidyverse, readr, ggplot2, corrplot)
# Change data name
db <- parte_a
getwd()
cd /Users/larubiano0/Desktop/andes/12do semestre/Ciencia de datos y econometría aplicada
cd "/Users/larubiano0/Desktop/andes/12do semestre/Ciencia de datos y econometría aplicada"
cd "/Users/larubiano0/Desktop/andes/12do semestre/Ciencia de datos y econometría aplicada
q
q()
clear
getwd()
cd "/Users/larubiano0/Desktop/andes/12do semestre/Ciencia de datos y econometría aplicada"
clear
setwd("~/Desktop/andes/12do semestre/Ciencia de datos y econometría aplicada/Taller1-CheMarket-Inc/scripts")
getwd()
clear
cd ..
# -----------------------------------------------------
# PART 1
# Exploratory and predictive analysis from observed data
# 0) Libraries and data
# 1) Description of users general behavior
# 2) Estimate effect of "sign_up" into "revenue"
# 3) Logistic regression of variables into sign_up
# r) Evaluating model's predictive capacity
# -----------------------------------------------------
# -----------------------------------------------------
# 0) Clean variables, libraries and data
# -----------------------------------------------------
# Libraries
require(pacman)
p_load(dplyr, tidyr, tidyverse, readr, ggplot2, corrplot)
# Change data name
db <- parte_a
cd ..
setwd("~/")
getwd()
setwd("~/Desktop/andes/12do semestre/Ciencia de datos y econometría aplicada/Taller1-CheMarket-Inc")
# -----------------------------------------------------
# PART 1
# Exploratory and predictive analysis from observed data
# 0) Libraries and data
# 1) Description of users general behavior
# 2) Estimate effect of "sign_up" into "revenue"
# 3) Logistic regression of variables into sign_up
# r) Evaluating model's predictive capacity
# -----------------------------------------------------
# -----------------------------------------------------
# 0) Clean variables, libraries and data
# -----------------------------------------------------
# Libraries
require(pacman)
p_load(dplyr, tidyr, tidyverse, readr, ggplot2, corrplot)
# Change data name
db <- parte_a
setwd("~/Desktop/andes/12do semestre/Ciencia de datos y econometría aplicada/Taller1-CheMarket-Inc/stores")
# -----------------------------------------------------
# PART 1
# Exploratory and predictive analysis from observed data
# 0) Libraries and data
# 1) Description of users general behavior
# 2) Estimate effect of "sign_up" into "revenue"
# 3) Logistic regression of variables into sign_up
# r) Evaluating model's predictive capacity
# -----------------------------------------------------
# -----------------------------------------------------
# 0) Clean variables, libraries and data
# -----------------------------------------------------
# Libraries
require(pacman)
p_load(dplyr, tidyr, tidyverse, readr, ggplot2, corrplot)
# Change data name
db <- parte_a
setwd("~/Desktop/andes/12do semestre/Ciencia de datos y econometría aplicada/Taller1-CheMarket-Inc")
# -----------------------------------------------------
# PART 0
# This script downloads data for both following parts
# 0) Good practices, clean variables and libraries
# 1) d
# 2) Save the data in a structure
# 3) Export the data to a .csv file
# -----------------------------------------------------
# -----------------------------------------------------
# 0) Good practices, clean variables and libraries
# -----------------------------------------------------
# Clean environment and libraries
rm(list = ls())
require(pacman)
p_load(rvest, dplyr, tidyr, readr, httr, jsonlite)
# -----------------------------------------------------
# PART A
# -----------------------------------------------------
# Load data for part A and b (relative path)
parte_a <- readRDS("stores/Parte_A.Rds")
# Verify no null data (dan todos 100k datos)
colSums(is.na(parte_a))
# Define categoric variable "device_type"
parte_a <- parte_a %>%
mutate(device_type = as.factor(device_type)) %>%
mutate(is_returning_user = as.factor(is_returning_user)) %>%
mutate(sign_up = as.factor(sign_up)) %>%
mutate(os_type = as.factor(os_type))
# Poner de base aquella categoría con menor cantidad de observaciones
parte_a$device_type <- relevel(parte_a$device_type, ref = "tablet")
# Poner de base aquella categoría con menor cantidad de observaciones
parte_a$device_type <- relevel(parte_a$os_type, ref = "other")
# -----------------------------------------------------
# PART B
# -----------------------------------------------------
parte_b <- readRDS("stores/Parte_B.Rds")
# Verify no null data
colSums(is.na(parte_b))
# Define categoric variable "device_type"
parte_b <- parte_b %>%
mutate(device_type = as.factor(device_type)) %>%
mutate(is_returning_user = as.factor(is_returning_user)) %>%
mutate(sign_up = as.factor(sign_up)) %>%
mutate(os_type = as.factor(os_type))
parte_b$device_type <- relevel(parte_b$device_type, ref = "tablet")
# -----------------------------------------------------
# PART 1
# Exploratory and predictive analysis from observed data
# 0) Libraries and data
# 1) Description of users general behavior
# 2) Estimate effect of "sign_up" into "revenue"
# 3) Logistic regression of variables into sign_up
# r) Evaluating model's predictive capacity
# -----------------------------------------------------
# -----------------------------------------------------
# 0) Clean variables, libraries and data
# -----------------------------------------------------
# Libraries
require(pacman)
p_load(dplyr, tidyr, tidyverse, readr, ggplot2, corrplot)
# Change data name
db <- parte_a
# See and keep variables
vars <- colnames(db)
# Get categoric and non-categoric variables
categoric_vars <- sapply(db, is.factor)
non_categoric_vars <- !categoric_vars
# -----------------------------------------------------
# 1) Description of users general behaviour
# -----------------------------------------------------
# Summary statistics
summary(db)
# TODO exportar
# Histograms (Para no categóricas) (#TODO: falta ponerlo bonito)
histograma_f <-  function(var_name) {
plot(density(db[[var_name]]), main = paste("Density of", var_name))
}
lapply(names(db)[non_categoric_vars], histograma_f)
# Log-Revenue y sqrt-time (vemos que si mejoran y lo cambiamos en la db)
#TODO: exportar log-rev para anexo
plot(density(log(db$Revenue)), main = "Density of log-Revenue")
plot(density(sqrt(db$time_spent)), main = "Density sqrt-time")
db <- db %>% mutate(log_revenue = log(Revenue))
db <- db %>% mutate(sqrt_time_spent = sqrt(time_spent))
### Ya verificamos las variables. Seleccionemos las dependientes del modelo
# See pairs (log(revenue) vs continuas) (correlations)
# TODO exportar
str <- paste0("~", paste(names(db)[non_categoric_vars], collapse = "+"))
pairs(db[, names(db)[non_categoric_vars]])
# Box-wiskers (Para no categóricas)
#TODO no sé si esto lo necesito realmente
# TODO exportar
box_f <-  function(var_name) {
boxplot(db[[var_name]], main = paste("Boxplot of", var_name))
}
lapply(names(db)[non_categoric_vars], box_f)
# Boxplots categóricas vs revenue
# Todo: gráficas bien, pero imprime un montón de números
# TODO exportar
box_f_cat_rev <-  function(var_name) {
boxplot(db$log_revenue ~ db[[var_name]], main = paste("Boxplot of log(Revenue) by", var_name))
}
lapply(names(db)[categoric_vars], box_f_cat_rev)
#
# Función: Boxplot + diferencia de medias
box_f_cat_rev <- function(var_name) {
# Boxplot
boxplot(db$log_revenue ~ db[[var_name]],
main = paste("Boxplot of log(Revenue) by", var_name))
# Diferencia de medias
means <- tapply(db$log_revenue, db[[var_name]], mean, na.rm = TRUE)
diffs <- combn(means, 2, FUN = function(x) diff(rev(x))) # diferencias entre categorías
# Graficar resultados con ggplot2 y ponlo bonito
df <- data.frame(Category = names(means), Mean = means)
ggplot(df, aes(x = Category, y = Mean)) +
geom_bar(stat = "identity") +
labs(title = paste("Mean of log(Revenue) by", var_name))
}
# Aplicar a todas las categóricas
lapply(names(db)[categoric_vars], box_f_cat_rev)
# Función: medias + IC95% y gráfico con barras + errorbars
plot_mean_diff <- function(var_name) {
# Calcular medias e IC95%
summary_df <- db %>%
group_by(!!sym(var_name)) %>%
summarise(
mean_rev = mean(log_revenue, na.rm = TRUE),
sd_rev   = sd(log_revenue, na.rm = TRUE),
n        = n(),
.groups = "drop"
) %>%
mutate(
se = sd_rev / sqrt(n),
ci_low = mean_rev - 1.96 * se,
ci_high = mean_rev + 1.96 * se
)
# Gráfico: barras + error bars en negro
ggplot(summary_df, aes(x = !!sym(var_name), y = mean_rev, fill = !!sym(var_name))) +
geom_col(width = 0.6, alpha = 0.8) +   # Barras
geom_errorbar(aes(ymin = ci_low, ymax = ci_high),
width = 0.2, color = "black", size = 0.8) +  # Intervalos en negro
labs(
title = paste("Media de log(Revenue) por", var_name),
x = var_name,
y = "Mean log(Revenue)"
) +
theme_minimal() +
theme(legend.position = "none")
}
# Aplicar la función a todas las variables categóricas
plots <- lapply(names(db)[categoric_vars], plot_mean_diff)
# Mostrar el primero como ejemplo
plots[[1]]
plots[[2]]
plots[[3]]
plots[[4]]
# TODO: exportar
# Categorics vs sign_up
# TODO exportar
cat_vs_cat <- function(var1, var2) {
ggplot(db, aes_string(x = var1, fill = var2)) +
geom_bar(position = "dodge") +
labs(title = paste("Count of", var2, "by", var1))
}
lapply(names(db)[categoric_vars], function(var1) {
cat_vs_cat(var1, "sign_up")
})
# -----------------------------------------------------
# 2) Estimate effect of "sign_up" into "revenue"
# -----------------------------------------------------
#TODO: si hay tiempo, puedo regresar usando lapply
# Simple linear regression model
model <- lm(log_revenue ~ sign_up, data = db)
# TODO exportar
summary(model)
# Controlling by all variables
model_controlled <- lm(log(Revenue) ~ sign_up + sqrt_time_spent + past_sessions +
device_type + is_returning_user + os_type, data = db)
# TODO exportar
summary(model_controlled)
# Controlar por interacciones entre vars fuertemente relacionadas a Revenue
# TODO: Rev vs time y vs past_Sessions sube al principio y luego baja
# Con device_type, os_type y sign_up no es tan claro; con returning_user si
model_controlled_revenue <- lm(log(Revenue) ~ sign_up + time_spent + past_sessions +
device_type + is_returning_user + os_type +
time_spent:past_sessions + time_spent:is_returning_user +
past_sessions:is_returning_user,
data = db)
summary(model_controlled_revenue)
# Controlar por interacciones con vars fuertemente relacionadas a sign_up
# TODO: sign vs device no parece tener (ver %'s), os_type si (ver %'s),
# faltaría ver continuas vs sign_up
model_controlled_sign <- lm(log(Revenue) ~ sign_up + time_spent + past_sessions +
device_type + is_returning_user + os_type +
sign_up:os_type, data = db)
summary(model_controlled_sign)
# Modelo final (quitando aquellas anteriores no significativas)
# -----------------------------------------------------
# 3) Logistic regression of variables into sign_up
# -----------------------------------------------------
# Quitamos revenue del set de variables explicativas
vars <- setdiff(names(db), c("sign_up", "revenue"))
# Fórmula dinámica
fml <- as.formula(paste("sign_up ~", paste(vars, collapse = " + ")))
# Ajustamos el modelo logístico
logit_model <- glm(fml, data = db, family = binomial(link = "logit"))
# Resumen del modelo
summary(logit_model)
# Odds ratios en vez de coeficientes logit
exp(coef(logit_model))
# Intervalos de confianza de los odds ratios
exp(confint(logit_model))
# -----------------------------------------------------
# 4) Evaluating model's predictive capacity
# -----------------------------------------------------
# Fijamos la semilla
set.seed(2025)
# Crear trainning data (30% of observations)
smp_size <- floor(0.3 * nrow(db))
# Creamos la columna de validación en la db para separar
validacion_ids <- sample(seq_len(nrow(db)), size = smp_size)
db$validacion <- 0
db$validacion[validacion_ids] <- 1
# test and trainning sets
data_test <- db %>% filter(validacion == 1)
data_train <- db %>% filter(validacion == 0)
# Crear parámetros y función k-fold
k <- 10
folds_i <- sample(rep(1:k, length.out = nrow(db)))
# Poner variables a los modelos para data_train
#TODO
models <- list(
c("sign_up"),
c("sign_up", "time_spent"),
c("sign_up", "time_spent", "past_sessions")
)
predictor<-function(regresors){
fmla<- formula(paste0("log(Revenue)",regresors))
model <- lm(fmla,data = data_train)
prediction_test <- predict(model, newdata = data_test)
mse<-with(data_test,mean((log(Revenue)-prediction_test)^2))
return(mse)
}
# Sacar MSE para los
lapply(models, predictor)
# Libraries
require(pacman)
p_load(dplyr, tidyr, tidyverse, readr, ggplot2, corrplot)
# Change data name
db <- parte_a
# See and keep variables
vars <- colnames(db)
# Get categoric and non-categoric variables
categoric_vars <- sapply(db, is.factor)
non_categoric_vars <- !categoric_vars
# -----------------------------------------------------
# 1) Description of users general behaviour
# -----------------------------------------------------
# Summary statistics
summary(db)
# Histograms (Para no categóricas) (#TODO: falta ponerlo bonito)
histograma_f <-  function(var_name) {
plot(density(db[[var_name]]), main = paste("Density of", var_name))
}
lapply(names(db)[non_categoric_vars], histograma_f)
# Log-Revenue y sqrt-time (vemos que si mejoran y lo cambiamos en la db)
#TODO: exportar log-rev para anexo
plot(density(log(db$Revenue)), main = "Density of log-Revenue")
plot(density(sqrt(db$time_spent)), main = "Density sqrt-time")
db <- db %>% mutate(log_revenue = log(Revenue))
db <- db %>% mutate(sqrt_time_spent = sqrt(time_spent))
